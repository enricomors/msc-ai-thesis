\documentclass[a4paper,singleside,12pt]{report} % Uncomment this for single side pdf.
%\documentclass[a4paper,twoside,12pt]{report} % Uncomment this for printing.

\usepackage{ai_bo_thesis}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}   % For equations
\usepackage{lmodern}
\usepackage{hyperref}  % Fixes \href errors
\usepackage{xurl}      % Allows line breaks in URLs
\usepackage{booktabs}  % Fixes \toprule, \midrule, \bottomrule errors
\setlength{\headheight}{14.5pt}  % Fixes fancyhdr warning

\usepackage{csquotes} % Recommended for proper quotation handling
\usepackage[backend=biber, style=numeric, sorting=nty, firstinits=true]{biblatex}
\addbibresource{biblio.bib}

\setmainfont{Times New Roman}
\begin{document}

\title{Hardware Dimensioning for Environmental Sustainability: benchmark of AI algorithms and environmental impact}
\topic{Intelligent Systems}
\candidate{Enrico Morselli}
\supervisor{Prof.~Andrea Borghesi}
\cosupervisor{& Allegra De Filippo, PhD.} % One co-supervisor.
%	\cosupervisors{& Dott.~Ing.~Luigi Bianchi\\& Dott~Avv.~Lucia Rossi} % More than one co-supervisor.
\academicyear{2024-2025}
\session{5th}

\frontispiece 
\dedication{dedicated(X) :- friend(X).}
\toc
\figstoc
\tablestoc
\begintext

\chapter{Introduction}

\section{Background and Rationale}

In recent years we have witnessed a dramatic increase in the performance of Artificial Intelligence technologies. Even if
AI still fails to exeed human ability in some complex cognitive tasks, as of 2023 it has surpassed human capabilites in a
range of tasks, such as image classification, basic reading comprehension, visual reasoning and natural language inference
\cite{AIIndexReport}. Not to mention the astonishing results achieved by Generative AI in tasks as Image and Video Generation
\cite{AIIndexReport}. This great advances in performance were made possible by a massive upscale of model sizes and 
computational resources ("compute" in short) dedicated to training state-of-the-art AI models. Research shows that for 
frontier AI models (i.e. those that were in the top 10 of training compute when they were released), the training compute has 
grown by a factor of 4-5x/year since 2010 \cite{epoch2024compute}. This surge in required compute has driven a corresponding 
spike in energy consumption for AI, and consequently, an higher environmental impact due to $\mathrm{CO_2}$ emissions.
For instance, for trainining their LLaMA models, Meta AI researchers have estimated a period of approximately 5 months of 
on $2048$ $\mathrm{A}100$ $80\mathrm{GB}$ GPUs, resulting in a total of $2,638 \mathrm{MWh}$ of energy and a total emission of 
$1,015$ $\mathrm{tCO_2eq}$ \cite{touvron2023llama}. Given the widespread application, the steep increase in model size and
complexity and the crescent energy requirements for AI applications, the Carbon Footprint of AI has become a growing concern
in the context of the current climate emergency.

In this work, we will explore an approach for addressing the issue of AI sustainability, by means of HADA (HArdware 
Dimensioning for AI Algorithms), which is a framework that uses ML to learn the relationship between an algorithm 
configuration and performance metrics, like total runtime, solution cost and memory usagem and then uses Optimization to find 
the best  Hardware architecture and its configuration to run an algorithm under required performances and budget limits
which is the problem known as Hardware Dimensioning \cite{DEFILIPPO2022109199}. What we will do is to extend this framework
in order to consider also the performance of the algorithms in terms of Energy consumption and Carbon Emissions, so that, 
ideally, we could find the best Algorithm and Hardware configuration that reduces the Carbon Footprint of computation. We 
will then proceed to test this approach on some small-scale algorithms that we could easily execute in a timely manner 
on local machines and HPC clusters.

The rest of the work is structured as follows:
\begin{itemize}
    \item \textbf{Chapter 2} Introduces Related works that addressed the issue of AI's carbon footprint, and how Carbon Footprint is determined
    \item \textbf{Chapter 3} Introduces some theoretical background about HADA, and explains the integration of the new metrics.
    \item \textbf{Chapter 4} Presents the experimental setup and the results of the experiments
    \item \textbf{Chapter 5} Presents the HADA framework, providing an overview of the tool
    \item \textbf{Chapter 6} Presents the conclusions
\end{itemize}

\chapter{Related Works}

\section{Sustainability in AI}

The environmental impact of training large AI models is illustrated by recent empirical assessments. 
Strubell et al. (2019) quantified the CO2 emissions of several NLP models and found that training a big
transformer with extensive hyperparameter tuning (including neural architecture search) emitted roughly 
626,000 pounds of $\mathrm{CO_2}$ - about the same as the lifetime emissions of five cars \cite{strubell2019energy}
\cite{dodge2022carbon}. These findings brought attention to the fact that accuracy gains in AI often come 
at a steep energy and carbon price. In response, researchers have begun to sistematically reporting energy
use and consequent $\mathrm{CO_2}$ emissions of model training to raise awareness \cite{dodge2022carbon} 
\cite{patterson2021carbon}. Beyond individual models, broader studies have examined AI's total energy and 
environmental footprint across the industry. Henderson et al. (2020) \cite{henderson2020carbon} introduced 
a framework for tracking real-time energy consumption and carbon emissions during ML experiments, encouraging
researchers to include these metrics in publications. Their work underscored that transparent reporting 
is essential for understanding and ultimately reducing AI's climate impacts. Industry-scale analyses also 
reveal sobering trends. Gupta et al. (2021) \cite{gupta2020carbon} analyzed the end-to-end footprint of 
computing and found that while operational emissions (from running hardware) have been partly curbed by 
efficiency improvements, the overall carbon footprint of computing continues to grow due to increased scale. 
Notably, they showed that for modern data centers and mobile devices, manufacturing and infrastructure 
(embodied carbon) now account for the majority of emissions. In other words, as data centers adopt cleaner 
power, the emissions “hidden” in hardware supply chains (chip fabrication, server manufacturing, etc.) 
become a dominant concern. Similarly, Wu et al. (2022) \cite{wu2021sustainableai} present a holistic study 
of AI at a large tech company (Meta/Facebook), examining the entire AI model lifecycle - from data processing 
and training to inference and hardware lifecycle. They report super-linear growth in AI workloads and 
infrastructure: for instance, daily inference operations doubled over a recent 3-year span, forcing a 2.5x
expansion in server capacity. Crucially, Wu et al. also highlight that embodied carbon is an increasing 
fraction of AI's total footprint, echoing that improvements in hardware efficiency alone cannot eliminate
AI's impact. Their analysis argues for looking beyond training alone - considering data center construction, 
supply chains, and the frequency of model retraining - to truly grasp AI's environmental impact.

A recurring theme in these studies is the diminishing return on energy investment for AI model improvements.
As models get larger and more complex, the incremental accuracy gains often require disproportionately more 
compute (and thus energy). Schwartz et al. (2020) \cite{GreenAI} dub the status quo “Red AI,” where 
researchers prioritize accuracy at almost any computational cost, and they note this trend is unsustainable 
both environmentally and even economically. Thompson et al. (2021) \cite{thompson2021deeplearning} similarly
observed that progress in benchmarks was coming with exponentially increasing computing cost, warning of 
diminishing returns and calling the situation unsustainable. Another challenge is equitable access: massive 
energy requirements make cutting-edge AI research expensive, potentially concentrating it in wealthy 
institutions and regions with robust infrastructure. This raises concerns that AI's growing energy hunger 
not only harms the planet but also exacerbates inequalities in who can afford to do top-tier research. These 
concerns have prompted calls for a paradigm shift toward “Green AI”, where efficiency and sustainability are 
treated as primary goals in model development. Several studies have addressed the issue of AI's carbon 
footprint. 

\section{Tools for tracking Carbon Emissions}

The growing awareness about AI's Carbon Footprint also motivated the development of dedicated tools and
methodologies to monitor the carbon footprint of AI workloads. A number of open-source tools and frameworks
have been created to help practitioners measure the energy consumption and $\mathrm{CO_2}$ emissions of their
code. ML $\mathrm{CO_2}$ Impact (Machine Learning Emissions Calculator), introduced by Lacoste et al. (2019),
\cite{lacoste2019quantifying} was one of the early tools for estimating emissions from model training. It is a 
web-based calculator where users input  information about their training run - such as hardware type (CPU/GPU), runtime, 
cloud provider, and location - and it computes the energy consumed and corresponding carbon emissions. 
The calculator leverages known power draw profiles of  hardware and regional carbon intensity factors. According to
\cite{bannour-etal-2021-evaluating}
the latest incarnation of this tool has evolved under the umbrella of the CodeCarbon initiative, integrating its 
functionality into CodeCarbon's open-source package.
    
\textbf{CodeCarbon} \cite{courty2024codecarbon}, which is the tool we used to expand HADA in this work, 
is an open-source Python package for tracking the carbon footprint  of computing projects. It 
integrates into ML code to log the resources used (CPU, GPU, etc.) and estimates the $\mathrm{CO_2}$ emissions
produced by the workload. Uniquely, CodeCarbon accounts for the location of the computation - using 
region-specific electricity carbon intensity data - to provide location-dependent emission estimates. 
This allows developers to see, for instance, that running the same training job on a low-carbon grid 
(e.g. hydro-powered Montreal) results in a much smaller footprint than running it on a coal-heavy grid. 
The goal is to inform and incentivize researchers and engineers to optimize or relocate their workloads to 
reduce emissions.

Cite other works which provides tools to track carbon emissions, e.g. LLMCarbon 
\cite{faiz2024llmcarbon}.

\chapter{Metodology}

\section{Empirical Model Learning in HADA}

HADA is based on \textbf{Empirical Model Learning (EML)}, \cite{LOMBARDI2017343} a framework allows the integration of ML 
models within an optimization problem to enable decision making over complex real-world systems. Hardware Dimensioning is
indeed a complex problem, since knowing the effect of algorithm configuration and Hardware settings on the algorithm performance
beforehand is not trivial, even for experts. For this reason, formalising this problem to express it as an optimisation problem
might be infeasible, due to this complexity. The idea is to learn, rather than directly express, the relationships between 
algorithm performance and HW resources via ML models, based on \textit{empirical} knowledge.
Broadly speaking, EML deals with solving declarative optimisation models with a complex component, $h$, which represents 
the relation between variables which can be acted upon (the decision variables, also called \textit{decidables}) $x$ and 
the \textit{observables} $y$ related to the system considered; 
the function $h(x)=y$ describes this relationships. As the $h$ component is complex, we cannot 
optimise directly over it. Hence, we exploit empirical knowledge to build a surrogate model $h_{\theta}$ learned from data, 
where $\theta$ is the parameter vector. In our case, we would like to learn the relationships between HW requirements and
AI algorithm performances, which would be very complex to express formally in a suitable model.

We present here the general mathematical formulation of EML:

\begin{align}
    \min \quad & f(x, y) \label{eq:objective} \\
    \text{s.t.} \quad & h_{\theta}(x) = y \label{eq:surrogate} \\
    & g_j(x, y) \quad \forall j \in J \label{eq:constraints} \\
    & x_i \in D_i \quad \forall x_i \in x \label{eq:domain}
\end{align}

where $x$ is the decision variables vector (each $x_i$ has its own domain $D_i$) and $y$ is the vector of observed variables.
 The goal is to minimize the objective function $f(x,y)$, which might depend on both decision and observed variables. Both 
 decision and observed variables can be subject to a set of constraints $g_j(x,y)$, such as classical inequalities from 
 Mathematical Programming and combinatorial predicates from Constraint Programming (e.g., global constraints). The function 
 $h_{\theta}(x)$ represents the approximate behaviour of the complex relation between decision and observed variables, and it 
 is, in practice, the encoding of a ML model.

The surrogate model $h_{\theta}(x)$ is trained as in the typical Supervised Learning setting; EML requires a training set 
$\mathcal{S} = (x_i, h(x_i))_{i=1}^{m}$ which is then used to find the parameter vector $\theta$ minimizing a loss function:

\begin{equation}
    \arg\min_{\theta} \frac{1}{m} \sum_{i=1}^{m} L(h_{\theta}(x_i), y_i^*)
\end{equation}

where $y_i^*$ are the ground-truth labels (or targets, in case of regression) of each data point in the training set 
$\mathcal{S}$ and $L$ is the loss function (e.g., $L_1$ or $L_2$ loss for scalar $y$'s).

The way in which HADA works can then be summarized in three main phases:
\begin{enumerate}
    \item \textit{Data Set Collection} - an initial phase to collect the data set by running multiple times the target algorithms, 
    under different configurations;
    \item \textit{Surrogate Model Creation} - once a training set is available, a set of ML models is then trained on such data, and 
    then these models are encoded as a set of variables and constraints following EML paradigm;
    \item \textit{Optimization} - post the user-defined constraints and objective function on top of the combinatorial structure
    formed by the encoded ML models and the domain-knowledge constraints, and finally solve the optimization model 
    (either until an optimal solution or a time limit is reached).
\end{enumerate}

\section{Integration of CodeCarbon in HADA}

The first step to extend HADA with Carfon Intensity is therefore to gather a dataset which includes data about carbon footprint 
of the algorithms execution. To do so, we can extend the code for running our algorithms with Codecarbon [https://codecarbon.io/], 
which is a package that enables developers to track emissions of their code, in order to estimate the carbon footprint. 
The carbon footprint of an algorithm depends on two factors: the energy needed to run it and the pollutants emitted when producing 
such energy. The former depends on the computing resources used (e.g. number of cores, running time, data centre efficiency) 
while the latter, called carbon intensity, depends on the location and production methods used (e.g. nuclear, gas or coal). 
The Carbon Intensity is expressed in terms of carbon dioxide equivalent ($\mathsf{CO_2e}$), and summarises the global warming 
effect of the Greenhouse Gases (GHG) emitted in the determined timeframe. [Green Algorithms]. The carbon footprint can then 
be computed with the following formula [Codecarbon docs]:

\begin{equation}
    \mathsf{C} = \mathsf{E} \times \mathsf{CI}
\end{equation}

Where:

\begin{itemize}
    \item $\mathsf{E}$ is the energy consumed by the computational infrastructure: quantified as kilowatt-hours (kWh).
    \item $\mathsf{CI}$ is the Carbon Intensity of the electricity consumed for computation: quantified as g of \(\mathsf{CO_2e}\) 
    emitted per kilowatt-hour of electricity.
\end{itemize}

Codecarbon directly measures the energy consumption of the CPU, GPU and RAM on which the code is executed at intervals of 15 sec 
(by default, but it can also be modified). It also monitor the duration of code execution to compute the total electricity 
consumption. Then it retrieves information about the carbon intensity of the electricity in your geographic area based on the 
location. Carbon Intensity of the consumed electricity is calculated as a weighted average of the emissions from the different
energy sources that are used to generate electricity, including fossil fuels and renewables. Based on the mix of energy sources 
in the local grid, this package calculates the Carbon Intensity of the electricity consumed.

To extend HADA for sustainable AI, we integrate CodeCarbon to track emissions in:
\begin{itemize}
\item ANTICIPATE and CONTINGENCY algorithms.
\item MaxFlow Algorithms:
\begin{itemize}
\item Boykov-Kolmogorov (BK)
\item Excess Incremental Breadth First Search (EIBFS)
\item Hochbaum's Pseudo Flow (HPF)
\end{itemize}
\end{itemize}

\chapter{Experimental Analysis}

\section{Benchmarking on Different Hardware Platforms}
Experiments were conducted on:
\begin{itemize}
\item MacBook Pro (2019)
\item Leonardo Supercomputer (CINECA HPC)
\end{itemize}

Each algorithm was executed on 30 instances with hyperparameter values ranging from 1 to 100, 
generating datasets with 6,000 records per algorithm per hardware platform.

\chapter{HADA-as-a-Service}

\section{HADA Web Application}
Benchmark data was integrated into the HADA web application, requiring:
\begin{itemize}
\item Creation of JSON configuration files for each algorithm-hardware combination.
\item Specification of hyperparameters and performance targets.
\end{itemize}

Example JSON structure:
\begin{verbatim}
{
"name": "anticipate",
"HW_ID": "macbook",
"hyperparams": [
{"ID": "num_scenarios", "type": "int", "LB": 1, "UB": 100}
],
"targets": [
{"ID": "time", "LB": null, "UB": null},
{"ID": "memory", "LB": null, "UB": null},
{"ID": "emissions", "LB": null, "UB": null}
]
}
\end{verbatim}

\chapter{Conclusions}

This work extends HADA by integrating carbon emission constraints, enhancing its applicability 
for sustainable AI hardware selection. Through experimental benchmarks on laptops and HPC systems, 
we validated the framework’s ability to balance performance and environmental impact. The web-based prototype 
enables users to make informed decisions when configuring AI workloads under sustainability constraints.

\appendix

\printbibliography[heading=bibintoc] % biblatex

\acknowledgements
I'm very grateful to the inventor of the Prolog language, without whom this thesis couldn't exist. I'd also like 
to acknowledge my advisor Prof. Mario Rossi by tail-recursively acknowledging my advisor.
	
\end{document}