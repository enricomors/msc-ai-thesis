\documentclass[a4paper,singleside,12pt]{report} % Uncomment this for single side pdf.
%\documentclass[a4paper,twoside,12pt]{report} % Uncomment this for printing.

\usepackage{ai_bo_thesis}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{hyperref}  % Fixes \href errors
\usepackage{xurl}      % Allows line breaks in URLs
\usepackage{booktabs}  % Fixes \toprule, \midrule, \bottomrule errors
\setlength{\headheight}{14.5pt}  % Fixes fancyhdr warning

\usepackage{csquotes} % Recommended for proper quotation handling
\usepackage[backend=biber, style=numeric, sorting=nty, firstinits=true]{biblatex}
\addbibresource{biblio.bib}

\setmainfont{Times New Roman}
\begin{document}

\title{Hardware Dimensioning for Environmental Sustainability: benchmark of AI algorithms and environmental impact}
\topic{Intelligent Systems}
\candidate{Enrico Morselli}
\supervisor{Prof.~Andrea Borghesi}
\cosupervisor{& Allegra De Filippo, PhD.} % One co-supervisor.
%	\cosupervisors{& Dott.~Ing.~Luigi Bianchi\\& Dott~Avv.~Lucia Rossi} % More than one co-supervisor.
\academicyear{2024-2025}
\session{5th}

\frontispiece 
\dedication{dedicated(X) :- friend(X).}
\toc
\figstoc
\tablestoc
\begintext

\chapter{Introduction}

\section{Background and Rationale}

In recent years we have witnessed a dramatic increase in the performance of Artificial Intelligence technologies. Even if
AI still fails to exeed human ability in some complex cognitive tasks, as of 2023 it has surpassed human capabilites in a
range of tasks, such as image classification, basic reading comprehension, visual reasoning and natural language inference
\cite{AIIndexReport}. Not to mention the astonishing results achieved by Generative AI in tasks as Image and Video Generation
\cite{AIIndexReport}. This great advances in performance were made possible by a massive upscale of model sizes and 
computational resources ("compute" in short) dedicated to training state-of-the-art AI models. Research shows that for 
frontier AI models (i.e. those that were in the top 10 of training compute when they were released), the training compute has 
grown by a factor of 4-5x/year since 2010 \cite{epoch2024compute}. This surge in required compute has driven a corresponding 
spike in energy consumption for AI, and consequently, an higher environmental impact due to $\mathrm{CO_2}$ emissions.
For instance, for trainining their LLaMA models, Meta AI researchers have estimated a period of approximately 5 months of 
on $2048$ $\mathrm{A}100$ $80\mathrm{GB}$ GPUs, resulting in a total of $2,638 \mathrm{MWh}$ of energy and a total emission of 
$1,015$ $\mathrm{tCO_2eq}$. Moreover, training is not the only source of emissions, has also the inference phase has a cost
in terms of energy, not to talk about the embodied emissions resulting from the manifacturing of Hardware components. 
The environmental impact of AI thus become a growing concern, with researchers beginning to sistematically reporting energy
use and consequent $\mathrm{CO_2}$ emissions of their models. 

In this work, we will explore an approach for addressing the issue of AI sustainability, by means of HADA (HArdware 
Dimensioning for AI Algorithms), which is a framework that uses ML to learn the relationship between an algorithm 
configuration and performance metrics, like total runtime, solution cost and memory usagem and then uses Optimization to find 
the best  Hardware architecture and its configuration to run an algorithm under required performances and budget limits
which is the problem known as Hardware Dimensioning \cite{DEFILIPPO2022109199}. What we will do is to extend this framework
in order to consider also the performance of the algorithms in terms of Energy consumption and Carbon Emissions, so that, 
ideally, we could find the best Algorithm and Hardware configuration that reduces the Carbon Footprint of computation. We 
will then proceed to test this approach on some small-scale algorithms that we could easily execute in a timely manner 
on local machines and HPC clusters.

The rest of the work is structured as follows:
\begin{itemize}
    \item \textbf{Chapter 2} Introduces Related works that addressed the issue of AI's carbon footprint, and how Carbon Footprint is determined
    \item \textbf{Chapter 3} Introduces some theoretical background about HADA, and explains the integration of the new metrics.
    \item \textbf{Chapter 4} Presents the experimental setup and the results of the experiments
    \item \textbf{Chapter 5} Presents the HADA framework, providing an overview of the tool
    \item \textbf{Chapter 6} Presents the conclusions
\end{itemize}

\chapter{Related Works}
\section{Carbon Footprint in AI}
Several studies have addressed the issue of AI’s carbon footprint. Tools like \textbf{Green Algorithms} and 
\textbf{CodeCarbon} have been developed to estimate and monitor emissions.

\textbf{CodeCarbon} is an open-source tool designed to track the energy consumption of computational resources 
and estimate the corresponding carbon emissions. The formula used is:

\begin{equation}
CO2eq = C \times E
\end{equation}

where:
\begin{itemize}
\item \textbf{C} represents the carbon intensity of electricity (kg CO2e per kWh), varying by country and energy mix.
\item \textbf{E} is the total electricity consumed during computation (kWh).
\end{itemize}

By monitoring CPU, GPU, and RAM consumption, CodeCarbon estimates the total emissions associated with a computation. 
It retrieves the carbon intensity based on the geographical location and logs results at user-defined intervals 
(default: 15 seconds).

Installation:
\begin{verbatim}
pip install codecarbon
\end{verbatim}

\chapter{Metodology}

\section{Empirical Model Learning in HADA}

HADA employs the \textbf{Empirical Model Learning (EML)} paradigm, which integrates \textbf{Machine Learning (ML)} models 
into an optimization framework. EML involves:

\begin{enumerate}
\item \textbf{Data Collection}: Running target algorithms under various hyperparameter configurations and hardware settings 
to collect performance data.
\item \textbf{Surrogate Model Creation}: Training ML models (e.g., Decision Trees) to approximate the relationship between 
input configurations and performance metrics (e.g., runtime, memory, carbon emissions).
\item \textbf{Optimization}: Using the learned models within a combinatorial optimization framework to find the best hardware
 configuration.
\end{enumerate}

HADA was originally applied to the \textbf{ANTICIPATE} and \textbf{CONTINGENCY} stochastic algorithms used in energy 
management. These algorithms compute energy production schedules while minimizing cost and considering uncertainties.

\section{Integration of CodeCarbon in HADA}

To extend HADA for sustainable AI, we integrate CodeCarbon to track emissions in:
\begin{itemize}
\item ANTICIPATE and CONTINGENCY algorithms.
\item MaxFlow Algorithms:
\begin{itemize}
\item Boykov-Kolmogorov (BK)
\item Excess Incremental Breadth First Search (EIBFS)
\item Hochbaum's Pseudo Flow (HPF)
\end{itemize}
\end{itemize}

\chapter{Experimental Analysis}

\section{Benchmarking on Different Hardware Platforms}
Experiments were conducted on:
\begin{itemize}
\item MacBook Pro (2019)
\item Leonardo Supercomputer (CINECA HPC)
\end{itemize}

Each algorithm was executed on 30 instances with hyperparameter values ranging from 1 to 100, 
generating datasets with 6,000 records per algorithm per hardware platform.

\chapter{HADA-as-a-Service}

\section{HADA Web Application}
Benchmark data was integrated into the HADA web application, requiring:
\begin{itemize}
\item Creation of JSON configuration files for each algorithm-hardware combination.
\item Specification of hyperparameters and performance targets.
\end{itemize}

Example JSON structure:
\begin{verbatim}
{
"name": "anticipate",
"HW_ID": "macbook",
"hyperparams": [
{"ID": "num_scenarios", "type": "int", "LB": 1, "UB": 100}
],
"targets": [
{"ID": "time", "LB": null, "UB": null},
{"ID": "memory", "LB": null, "UB": null},
{"ID": "emissions", "LB": null, "UB": null}
]
}
\end{verbatim}

\chapter{Conclusions}

This work extends HADA by integrating carbon emission constraints, enhancing its applicability 
for sustainable AI hardware selection. Through experimental benchmarks on laptops and HPC systems, 
we validated the framework’s ability to balance performance and environmental impact. The web-based prototype 
enables users to make informed decisions when configuring AI workloads under sustainability constraints.

\appendix

\printbibliography[heading=bibintoc] % biblatex

\acknowledgements
I'm very grateful to the inventor of the Prolog language, without whom this thesis couldn't exist. I'd also like 
to acknowledge my advisor Prof. Mario Rossi by tail-recursively acknowledging my advisor.
	
\end{document}