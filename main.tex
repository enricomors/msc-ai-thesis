\documentclass[a4paper,singleside,12pt]{report} % Uncomment this for single side pdf.
%\documentclass[a4paper,twoside,12pt]{report} % Uncomment this for printing.

\usepackage{ai_bo_thesis}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{hyperref}  % Fixes \href errors
\usepackage{xurl}      % Allows line breaks in URLs
\usepackage{booktabs}  % Fixes \toprule, \midrule, \bottomrule errors
\setlength{\headheight}{14.5pt}  % Fixes fancyhdr warning

\usepackage{csquotes} % Recommended for proper quotation handling
\usepackage[backend=biber, style=numeric, sorting=nty, firstinits=true]{biblatex}
\addbibresource{biblio.bib}

\setmainfont{Times New Roman}
\begin{document}

\title{Hardware Dimensioning for Environmental Sustainability: benchmark of AI algorithms and environmental impact}
\topic{Intelligent Systems}
\candidate{Enrico Morselli}
\supervisor{Prof.~Andrea Borghesi}
\cosupervisor{& Allegra De Filippo, PhD.} % One co-supervisor.
%	\cosupervisors{& Dott.~Ing.~Luigi Bianchi\\& Dott~Avv.~Lucia Rossi} % More than one co-supervisor.
\academicyear{2024-2025}
\session{5th}

\frontispiece 
\dedication{dedicated(X) :- friend(X).}
\toc
\figstoc
\tablestoc
\begintext

\chapter{Introduction}

\section{Background and Rationale}

In recent years we have witnessed a dramatic increase in the performance of Artificial Intelligence technologies. Even if
AI still fails to exeed human ability in some complex cognitive tasks, as of 2023 it has surpassed human capabilites in a
range of tasks, such as image classification, basic reading comprehension, visual reasoning and natural language inference
\cite{AIIndexReport}. Not to mention the astonishing results achieved by Generative AI in tasks as Image and Video Generation
\cite{AIIndexReport}. This great advances in performance were made possible by a massive upscale of model sizes and 
computational resources ("compute" in short) dedicated to training state-of-the-art AI models. Research shows that for 
frontier AI models (i.e. those that were in the top 10 of training compute when they were released), the training compute has 
grown by a factor of 4-5x/year since 2010 \cite{epoch2024compute}. This surge in required compute has driven a corresponding 
spike in energy consumption for AI, and consequently, an higher environmental impact due to $\mathrm{CO_2}$ emissions.
For instance, for trainining their LLaMA models, Meta AI researchers have estimated a period of approximately 5 months of 
on $2048$ $\mathrm{A}100$ $80\mathrm{GB}$ GPUs, resulting in a total of $2,638 \mathrm{MWh}$ of energy and a total emission of 
$1,015$ $\mathrm{tCO_2eq}$ \cite{touvron2023llama}. Given the widespread application, the steep increase in model size and
complexity and the crescent energy requirements for AI applications, the Carbon Footprint of AI has become a growing concern
in the context of the current climate emergency.

In this work, we will explore an approach for addressing the issue of AI sustainability, by means of HADA (HArdware 
Dimensioning for AI Algorithms), which is a framework that uses ML to learn the relationship between an algorithm 
configuration and performance metrics, like total runtime, solution cost and memory usagem and then uses Optimization to find 
the best  Hardware architecture and its configuration to run an algorithm under required performances and budget limits
which is the problem known as Hardware Dimensioning \cite{DEFILIPPO2022109199}. What we will do is to extend this framework
in order to consider also the performance of the algorithms in terms of Energy consumption and Carbon Emissions, so that, 
ideally, we could find the best Algorithm and Hardware configuration that reduces the Carbon Footprint of computation. We 
will then proceed to test this approach on some small-scale algorithms that we could easily execute in a timely manner 
on local machines and HPC clusters.

The rest of the work is structured as follows:
\begin{itemize}
    \item \textbf{Chapter 2} Introduces Related works that addressed the issue of AI's carbon footprint, and how Carbon Footprint is determined
    \item \textbf{Chapter 3} Introduces some theoretical background about HADA, and explains the integration of the new metrics.
    \item \textbf{Chapter 4} Presents the experimental setup and the results of the experiments
    \item \textbf{Chapter 5} Presents the HADA framework, providing an overview of the tool
    \item \textbf{Chapter 6} Presents the conclusions
\end{itemize}

\chapter{Related Works}

\section{Sustainability in AI}

The environmental impact of training large AI models is illustrated by recent empirical assessments. 
Strubell et al. (2019) quantified the CO2 emissions of several NLP models and found that training a big
transformer with extensive hyperparameter tuning (including neural architecture search) emitted roughly 
626,000 pounds of $\mathrm{CO_2}$ - about the same as the lifetime emissions of five cars \cite{strubell2019energy}
\cite{dodge2022carbon}. These findings brought attention to the fact that accuracy gains in AI often come 
at a steep energy and carbon price. In response, researchers have begun to sistematically reporting energy
use and consequent $\mathrm{CO_2}$ emissions of model training to raise awareness \cite{dodge2022carbon} 
\cite{patterson2021carbon}. Beyond individual models, broader studies have examined AI's total energy and 
environmental footprint across the industry. Henderson et al. (2020) \cite{henderson2020carbon} introduced 
a framework for tracking real-time energy consumption and carbon emissions during ML experiments, encouraging
researchers to include these metrics in publications. Their work underscored that transparent reporting 
is essential for understanding and ultimately reducing AI's climate impacts. Industry-scale analyses also 
reveal sobering trends. Gupta et al. (2021) \cite{gupta2020carbon} analyzed the end-to-end footprint of 
computing and found that while operational emissions (from running hardware) have been partly curbed by 
efficiency improvements, the overall carbon footprint of computing continues to grow due to increased scale. 
Notably, they showed that for modern data centers and mobile devices, manufacturing and infrastructure 
(embodied carbon) now account for the majority of emissions. In other words, as data centers adopt cleaner 
power, the emissions “hidden” in hardware supply chains (chip fabrication, server manufacturing, etc.) 
become a dominant concern. Similarly, Wu et al. (2022) \cite{wu2021sustainableai} present a holistic study 
of AI at a large tech company (Meta/Facebook), examining the entire AI model lifecycle - from data processing 
and training to inference and hardware lifecycle. They report super-linear growth in AI workloads and 
infrastructure: for instance, daily inference operations doubled over a recent 3-year span, forcing a 2.5x
expansion in server capacity. Crucially, Wu et al. also highlight that embodied carbon is an increasing 
fraction of AI's total footprint, echoing that improvements in hardware efficiency alone cannot eliminate
AI's impact. Their analysis argues for looking beyond training alone - considering data center construction, 
supply chains, and the frequency of model retraining - to truly grasp AI's environmental impact.

A recurring theme in these studies is the diminishing return on energy investment for AI model improvements.
As models get larger and more complex, the incremental accuracy gains often require disproportionately more 
compute (and thus energy). Schwartz et al. (2020) \cite{GreenAI} dub the status quo “Red AI,” where 
researchers prioritize accuracy at almost any computational cost, and they note this trend is unsustainable 
both environmentally and even economically. Thompson et al. (2021) \cite{thompson2021deeplearning} similarly
observed that progress in benchmarks was coming with exponentially increasing computing cost, warning of 
diminishing returns and calling the situation unsustainable. Another challenge is equitable access: massive 
energy requirements make cutting-edge AI research expensive, potentially concentrating it in wealthy 
institutions and regions with robust infrastructure. This raises concerns that AI's growing energy hunger 
not only harms the planet but also exacerbates inequalities in who can afford to do top-tier research. These 
concerns have prompted calls for a paradigm shift toward “Green AI”, where efficiency and sustainability are 
treated as primary goals in model development. Several studies have addressed the issue of AI's carbon 
footprint. 

\section{Tools for tracking Carbon Emissions}

The growing awareness about AI's Carbon Footprint also motivated the development of dedicated tools and
methodologies to monitor the carbon footprint of AI workloads. A number of open-source tools and frameworks
have been created to help practitioners measure the energy consumption and $\mathrm{CO_2}$ emissions of their
code. Among those tools, there is \textbf{CodeCarbon}, which is the tool we used to expand HADA in this work.
CodeCarbon is an open-source Python package for tracking the carbon footprint  of computing projects. It 
integrates into ML code to log the resources used (CPU, GPU, etc.) and estimates the $\mathrm{CO_2}$ emissions
produced by the workload. Uniquely, CodeCarbon accounts for the location of the computation - using 
region-specific electricity carbon intensity data - to provide location-dependent emission estimates. 
This allows developers to see, for instance, that running the same training job on a low-carbon grid 
(e.g. hydro-powered Montreal) results in a much smaller footprint than running it on a coal-heavy grid. 
The goal is to inform and incentivize researchers and engineers to optimize or relocate their workloads to 
reduce emissions.

Tools like \textbf{Green Algorithms} and 
\textbf{CodeCarbon} have been developed to estimate and monitor emissions.

\textbf{CodeCarbon} is an open-source tool designed to track the energy consumption of computational resources 
and estimate the corresponding carbon emissions. The formula used is:

\begin{equation}
CO2eq = C \times E
\end{equation}

where:
\begin{itemize}
\item \textbf{C} represents the carbon intensity of electricity (kg CO2e per kWh), varying by country and energy mix.
\item \textbf{E} is the total electricity consumed during computation (kWh).
\end{itemize}

By monitoring CPU, GPU, and RAM consumption, CodeCarbon estimates the total emissions associated with a computation. 
It retrieves the carbon intensity based on the geographical location and logs results at user-defined intervals 
(default: 15 seconds).

Installation:
\begin{verbatim}
pip install codecarbon
\end{verbatim}

\chapter{Metodology}

\section{Empirical Model Learning in HADA}

HADA employs the \textbf{Empirical Model Learning (EML)} paradigm, which integrates \textbf{Machine Learning (ML)} models 
into an optimization framework. EML involves:

\begin{enumerate}
\item \textbf{Data Collection}: Running target algorithms under various hyperparameter configurations and hardware settings 
to collect performance data.
\item \textbf{Surrogate Model Creation}: Training ML models (e.g., Decision Trees) to approximate the relationship between 
input configurations and performance metrics (e.g., runtime, memory, carbon emissions).
\item \textbf{Optimization}: Using the learned models within a combinatorial optimization framework to find the best hardware
 configuration.
\end{enumerate}

HADA was originally applied to the \textbf{ANTICIPATE} and \textbf{CONTINGENCY} stochastic algorithms used in energy 
management. These algorithms compute energy production schedules while minimizing cost and considering uncertainties.

\section{Integration of CodeCarbon in HADA}

To extend HADA for sustainable AI, we integrate CodeCarbon to track emissions in:
\begin{itemize}
\item ANTICIPATE and CONTINGENCY algorithms.
\item MaxFlow Algorithms:
\begin{itemize}
\item Boykov-Kolmogorov (BK)
\item Excess Incremental Breadth First Search (EIBFS)
\item Hochbaum's Pseudo Flow (HPF)
\end{itemize}
\end{itemize}

\chapter{Experimental Analysis}

\section{Benchmarking on Different Hardware Platforms}
Experiments were conducted on:
\begin{itemize}
\item MacBook Pro (2019)
\item Leonardo Supercomputer (CINECA HPC)
\end{itemize}

Each algorithm was executed on 30 instances with hyperparameter values ranging from 1 to 100, 
generating datasets with 6,000 records per algorithm per hardware platform.

\chapter{HADA-as-a-Service}

\section{HADA Web Application}
Benchmark data was integrated into the HADA web application, requiring:
\begin{itemize}
\item Creation of JSON configuration files for each algorithm-hardware combination.
\item Specification of hyperparameters and performance targets.
\end{itemize}

Example JSON structure:
\begin{verbatim}
{
"name": "anticipate",
"HW_ID": "macbook",
"hyperparams": [
{"ID": "num_scenarios", "type": "int", "LB": 1, "UB": 100}
],
"targets": [
{"ID": "time", "LB": null, "UB": null},
{"ID": "memory", "LB": null, "UB": null},
{"ID": "emissions", "LB": null, "UB": null}
]
}
\end{verbatim}

\chapter{Conclusions}

This work extends HADA by integrating carbon emission constraints, enhancing its applicability 
for sustainable AI hardware selection. Through experimental benchmarks on laptops and HPC systems, 
we validated the framework’s ability to balance performance and environmental impact. The web-based prototype 
enables users to make informed decisions when configuring AI workloads under sustainability constraints.

\appendix

\printbibliography[heading=bibintoc] % biblatex

\acknowledgements
I'm very grateful to the inventor of the Prolog language, without whom this thesis couldn't exist. I'd also like 
to acknowledge my advisor Prof. Mario Rossi by tail-recursively acknowledging my advisor.
	
\end{document}