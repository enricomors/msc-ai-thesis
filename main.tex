\documentclass[a4paper,singleside,12pt]{report} % Uncomment this for single side pdf.
%\documentclass[a4paper,twoside,12pt]{report} % Uncomment this for printing.

\usepackage{ai_bo_thesis}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}   % For equations
\usepackage{lmodern}
\usepackage{hyperref}  % Fixes \href errors
\usepackage{xurl}      % Allows line breaks in URLs
\usepackage{booktabs}  % Fixes \toprule, \midrule, \bottomrule errors
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\setlength{\headheight}{14.5pt}  % Fixes fancyhdr warning

\usepackage{csquotes} % Recommended for proper quotation handling
\usepackage[backend=biber, style=numeric, sorting=nty, firstinits=true]{biblatex}
\addbibresource{biblio.bib}

\setmainfont{Times New Roman}
\begin{document}

\title{Hardware Dimensioning for Environmental Sustainability: benchmark of AI algorithms and environmental impact}
\topic{Intelligent Systems}
\candidate{Enrico Morselli}
\supervisor{Prof.~Andrea Borghesi}
\cosupervisor{& Prof.~Allegra De Filippo} % One co-supervisor.
%	\cosupervisors{& Dott.~Ing.~Luigi Bianchi\\& Dott~Avv.~Lucia Rossi} % More than one co-supervisor.
\academicyear{2024-2025}
\session{5th}

\frontispiece 
\dedication{dedicated(X) :- friend(X).}
\toc
\figstoc
\tablestoc
\begintext

\chapter{Introduction}

\section{Background and Rationale}

\section{Introduction}

In recent years, we have witnessed a dramatic improvement in the performance of Artificial Intelligence (AI) technologies. 
While AI still falls short of human ability in some complex cognitive tasks, as of 2023, it has surpassed human capabilities 
in various domains, including image classification, basic reading comprehension, visual reasoning, and natural language inference 
\cite{AIIndexReport}. Moreover, Generative AI has achieved astonishing results in areas such as image and video generation, further 
demonstrating the rapid advancements in the field \cite{AIIndexReport}. 

These remarkable gains in AI performance have been primarily driven by the large-scale expansion of model sizes and computational 
resources (often referred to as "compute") dedicated to training state-of-the-art AI models. Research indicates that, for frontier 
AI models—those that ranked among the top 10 in terms of training compute at the time of their release—the amount of compute used
for training has been increasing at an exponential rate, growing by a factor of 4-5x per year since 2010 \cite{epoch2024compute}. 

However, this surge in compute requirements has led to a corresponding increase in energy consumption, which, in turn, has 
amplified the environmental impact of AI due to $\mathsf{CO_2}$ emissions. For instance, the training of Meta AI's LLaMA models 
was estimated to take approximately five months on a cluster of $2048$ $\mathsf{A}100$ $80\mathsf{GB}$ GPUs, consuming a total 
of $2,638$ $\mathsf{MWh}$ of energy and producing approximately $1,015$ $\mathsf{tCO_2eq}$ in emissions \cite{touvron2023llama}. 
Given the growing adoption of AI technologies, the rapid increase in model size and complexity, and the escalating energy demands 
of AI applications, the carbon footprint of AI has become an increasingly pressing concern, particularly in the context of the 
ongoing climate emergency.

In this work, we explore a novel approach to addressing the sustainability challenges of AI by extending the HADA (HArdware 
Dimensioning for AI Algorithms) framework. HADA is a framework that leverages Machine Learning (ML) to learn the relationship 
between an algorithm's configuration and its performance metrics—such as total runtime, solution cost, and memory usage—and then 
employs optimization techniques to identify the optimal hardware architecture and configuration that meet predefined performance 
and budget constraints. This process, known as Hardware Dimensioning, has been studied as a means to efficiently allocate 
computational resources while maintaining performance requirements \cite{DEFILIPPO2022109199}. 

Our contribution extends HADA by incorporating energy consumption and carbon emissions as additional performance metrics. 
By doing so, we aim to identify the best combination of algorithm and hardware configuration that minimizes the environmental 
impact of computation. To validate our approach, we will conduct experiments on small-scale algorithms that can be executed in 
a timely manner on local machines and high-performance computing (HPC) clusters. Ultimately, we seek to demonstrate how AI 
workloads can be optimized not only for efficiency and cost but also for sustainability, thus contributing to the broader goal 
of reducing AI's carbon footprint.

% TODO: expand the "Rest of the work" section

The rest of the work is structured as follows:
\begin{itemize}
    \item \textbf{Chapter 2} Introduces Related works that addressed the issue of AI's carbon footprint, and how Carbon Footprint is determined
    \item \textbf{Chapter 3} Introduces some theoretical background about HADA, and explains the integration of the new metrics.
    \item \textbf{Chapter 4} Presents the experimental setup and the results of the experiments
    \item \textbf{Chapter 5} Presents the HADA framework, providing an overview of the tool
    \item \textbf{Chapter 6} Presents the conclusions
\end{itemize}

\chapter{Related Works}

\section{Sustainability in AI}

The environmental impact of training large AI models has been underscored by recent empirical assessments. Strubell et al. (2019)
quantified the $\mathsf{CO_2}$ emissions of several Natural Language Processing (NLP) models and discovered that training a large
transformer with extensive hyperparameter tuning, including neural architecture search, emitted approximately 626,000 pounds 
(around 284 metric tons) of $\mathsf{CO_2}$—comparable to the lifetime emissions of five cars \cite{strubell2019energy}. 
Another study found out that GPT-3 released 552 metric tons of $\mathsf{CO_2}$ into the atmosphere during training, the 
equivalent of 123 gasoline-powered passenger vehicles driven for one year \cite{patterson2021carbon}. These findings highlight that 
improvements in AI accuracy often come at a significant energy and carbon cost.

In response to these concerns, researchers have begun systematically reporting energy usage and the resulting $\mathsf{CO_2}$ 
emissions associated with model training to raise awareness \cite{dodge2022carbon, patterson2021carbon}. Transparent reporting 
is essential for understanding and ultimately mitigating AI's climate impacts. For instance, Henderson et al. (2020) introduced 
a framework for tracking real-time energy consumption and carbon emissions during Machine Learning (ML) experiments, encouraging 
researchers to include these metrics in their publications \cite{henderson2020carbon}.

Broader studies have examined AI's total energy and environmental footprint across the industry. Gupta et al. (2021) analyzed 
the end-to-end footprint of computing and found that while operational emissions (from running hardware) have been partly curbed 
by efficiency improvements, the overall carbon footprint of computing continues to grow due to increased scale. Notably, they 
showed that for modern data centers and mobile devices, manufacturing and infrastructure (embodied carbon) now account for the 
majority of emissions. As data centers adopt cleaner power, the emissions embedded in hardware supply chains—such as chip 
fabrication and server manufacturing—become a dominant concern. \cite{gupta2020carbon}

Similarly, Wu et al. (2022) conducted a comprehensive study of AI at a large technology company (Meta/Facebook), examining the 
entire AI model lifecycle—from data processing and training to inference and hardware lifecycle. They reported super-linear 
growth in AI workloads and infrastructure; for instance, daily inference operations doubled over a recent three-year span, 
necessitating a 2.5x expansion in server capacity. Crucially, Wu et al. also highlighted that embodied carbon is an increasing 
fraction of AI's total footprint, echoing that improvements in hardware efficiency alone cannot eliminate AI's impact. Their 
analysis argues for looking beyond training alone—considering data center construction, supply chains, and the frequency of 
model retraining—to truly grasp AI's environmental impact. \cite{wu2021sustainableai}

A recurring theme in these studies is the diminishing return on energy investment for AI model improvements. As models become 
larger and more complex, incremental accuracy gains often require disproportionately more compute power, and thus energy. 
Schwartz et al. (2020) termed this phenomenon "Red AI," where researchers prioritize accuracy regardless of computational cost, 
and noted this trend is unsustainable both environmentally and economically \cite{GreenAI}. Thompson et al. (2021) similarly 
observed that progress in benchmarks was accompanied by exponentially increasing computing costs, warning of diminishing returns 
and calling the situation unsustainable. \cite{thompson2021deeplearning}

Another challenge is equitable access: massive energy requirements make cutting-edge AI research expensive, potentially 
concentrating it in wealthy institutions and regions with robust infrastructure. This raises concerns that AI's growing energy 
demands not only harm the planet but also exacerbate inequalities in who can afford to conduct top-tier research. These concerns 
have prompted calls for a paradigm shift toward "Green AI," where efficiency and sustainability are treated as primary goals in 
model development.

In summary, the environmental sustainability of AI has become a critical area of concern. As the field advances, it is 
imperative to balance the pursuit of performance improvements with the need to minimize energy consumption and carbon emissions. 
This balance is essential not only for mitigating climate change but also for ensuring equitable access to AI research and 
development.

\section{Tools for Tracking Carbon Emissions}

The increasing awareness of AI’s carbon footprint has motivated the development of dedicated tools and methodologies for monitoring 
the environmental impact of AI workloads. A number of open-source tools and frameworks have been introduced to help practitioners 
measure the energy consumption and $\mathsf{CO_2}$ emissions of their code, enabling more informed decisions about sustainability 
in AI research and deployment.

One of the early tools for estimating emissions from model training was \textbf{ML CO2 Impact} (Machine Learning 
Emissions Calculator), introduced by Lacoste et al. (2019) \cite{lacoste2019quantifying}. This web-based calculator allows users 
to input key information about their training runs—such as hardware type (CPU/GPU), runtime, cloud provider, and location—and 
computes the corresponding energy consumption and carbon emissions. The tool relies on known power draw profiles of hardware 
components and regional carbon intensity factors to generate estimates. According to Bannour et al. (2021) 
\cite{bannour-etal-2021-evaluating}, the latest iteration of this tool has evolved under the umbrella of the \textbf{CodeCarbon} 
initiative, integrating its functionality into the open-source CodeCarbon package.

\textbf{CodeCarbon} \cite{courty2024codecarbon}, which is the tool we employ in this work to enhance HADA, is an open-source 
Python package designed to track the carbon footprint of computing projects. It integrates seamlessly into ML workflows, 
logging resource usage (CPU, GPU, and other components) and estimating the $\mathsf{CO_2}$ emissions produced by the workload. 
A key feature of CodeCarbon is its ability to account for the geographic location of the computation—leveraging region-specific 
electricity carbon intensity data to provide location-dependent emission estimates. \textbf{Carbon Intensity} refers to the 
amount of $\mathsf{CO_2}$ emitted per unit of electricity generated, typically measured in grams of $\mathsf{CO_2}$ per 
kilowatt-hour (gCO$_2$/kWh). This metric varies significantly across regions depending on the energy mix used for electricity 
generation. For example, a region relying primarily on coal-fired power plants will have a much higher carbon intensity compared 
to one powered predominantly by renewable sources such as hydropower or wind energy. By incorporating this factor, CodeCarbon 
enables users to assess and compare the emissions impact of running their workloads in different locations. This means that 
shifting computations to regions with lower-carbon grids can lead to substantial reductions in total $\mathsf{CO_2}$ emissions.
This feature allows researchers and engineers to assess the impact of their computational choices; for example, running the same 
training job on a hydro-powered grid (e.g., in Montreal) results in significantly lower emissions than executing it on a 
coal-reliant grid. By making these comparisons explicit, CodeCarbon aims to inform and incentivize practitioners to optimize or 
relocate their workloads in a manner that reduces emissions.

Considerable efforts have also been made to quantify the carbon footprint of \textbf{Large Language Models (LLMs)}, as these 
models have experienced the most dramatic growth in both complexity and size. While much attention has been given to the energy 
demands of training these models, recent studies have also highlighted the substantial energy costs incurred during inference—the 
phase where models generate responses based on input data. Husom et al. (2024) introduced MELODI (Monitoring Energy Levels and 
Optimization for Data-driven Inference), a comprehensive framework designed to monitor and analyze energy consumption during LLM 
inference processes \cite{husom2024priceprompting}. MELODI enables detailed observations of power consumption dynamics and 
facilitates the creation of a dataset reflective of energy efficiency across varied deployment scenarios. Their findings indicate 
substantial disparities in energy efficiency, suggesting ample scope for optimization and the adoption of sustainable measures 
in LLM deployment. Similarly, Samsi et al. (2023) conducted experiments to study the computational and energy utilization of 
LLM inference, focusing on different sizes of LLaMA—a state-of-the-art LLM developed by Meta AI—on two generations of popular 
GPUs (NVIDIA V100 and A100) \cite{samsi2023wordswattsbenchmarkingenergy}. They benchmarked inference performance and energy 
costs across diverse tasks and presented results of multi-node, multi-GPU inference using model sharding across up to 32 GPUs. 
Their work provides valuable insights into compute performance and energy utilization characteristics of LLM inference, 
highlighting the need for cost-saving measures, efficient hardware usage, and optimal deployment strategies. Faiz et al. (2024) 
\cite{faiz2024llmcarbon}, introduces a model to estimate the total emissions of an LLM, accounting for both its \textit{Operational} 
Carbon Footprint (energy consumption during training and inference) and its \textit{Embodied} Carbon Footprint (emissions from 
hardware production and infrastructure). This study also highlights the importance of \textbf{Data Center Energy Efficiency}, 
measured by the \textit{Power Usage Effectiveness (PUE)}, and the \textbf{Carbon Intensity of Electricity}—factors that reinforce 
the findings of CodeCarbon. Specifically, the study emphasizes that performing AI computations in regions where energy sources 
are cleaner (e.g., renewable-powered grids) can significantly reduce $\mathsf{CO_2}$ emissions. 

The development and adoption of these tracking tools play a crucial role in promoting sustainable AI research and deployment. 
By quantifying emissions and identifying optimization strategies, these tools provide the foundation for a more environmentally 
responsible approach to AI development.
 
\chapter{Methodology}

\section{Empirical Model Learning in HADA} \label{EML-HADA}

As introduced earlier, the core focus of this work is \textbf{Hardware Dimensioning}, which refers to the challenge of identifying 
the most suitable hardware configuration for executing a given algorithm while meeting predefined performance and budget constraints.
Specifically, given a target algorithm, our goal is to determine the optimal combination of algorithmic hyperparameters and hardware
architecture that ensures the algorithm operates efficiently under specific performance requirements. However, this is a non-trivial 
optimization problem, primarily due to the difficulty of predicting an AI algorithm’s performance across different hardware architectures 
and evaluating the impact of varying algorithmic and hardware configurations.

The key idea behind HADA is to integrate expert domain knowledge, such as execution time constraints, required solution quality, and budget 
limitations, with data-driven models that can infer performance relationships. At the foundation of HADA lies the \textbf{Empirical Model 
Learning (EML)} framework \cite{LOMBARDI2017343}, which employs Machine Learning (ML) techniques to construct a model of an optimization 
problem. Instead of explicitly formulating complex relationships between algorithm performance and hardware resources, EML leverages 
data-driven approximations, learning these dependencies empirically. These learned models are then embedded directly within the 
optimization process.

This approach is particularly advantageous in scenarios where deriving an exact mathematical formulation for the problem is infeasible 
due to its complexity. Broadly, EML facilitates the resolution of declarative optimization models that involve an intricate component, 
denoted as $h$, which captures the relationship between \textit{decision variables} (also called \textit{decidables}) $x$ and 
\textit{observables} $y$—i.e., measurable performance metrics of the system. This relationship is represented by the function $h(x) = y$. 
Since $h$ is often highly complex and not directly optimizable, an alternative approach is to learn an approximate model, denoted as 
$h_{\theta}$, where $\theta$ represents the set of learned parameters.

\subsection{Mathematical Formulation}

The general mathematical formulation of the EML framework can be expressed as follows:

\begin{align}
    \min \quad & f(x, y) \label{eq:objective} \\
    \text{s.t.} \quad & h_{\theta}(x) = y \label{eq:surrogate} \\
    & g_j(x, y) \quad \forall j \in J \label{eq:constraints} \\
    & x_i \in D_i \quad \forall x_i \in x \label{eq:domain}
\end{align}

where:
\begin{itemize}
    \item $x$ represents the vector of decision variables, each $x_i$ belonging to a domain $D_i$;
    \item $y$ denotes the vector of observed variables;
    \item The objective function $f(x, y)$, which depends on both decision and observed variables, is to be minimized;
    \item Constraints $g_j(x, y)$ enforce feasibility conditions on the variables, which can include classical mathematical programming 
    inequalities and combinatorial constraints from Constraint Programming;
    \item The function $h_{\theta}(x)$ serves as an approximation of the complex relationship between decision and observed variables, and 
    it is instantiated as a trained ML model.
\end{itemize}

The surrogate model $h_{\theta}(x)$ is learned through a standard supervised learning procedure. Given a training set $\mathcal{S} 
= \{(x_i, h(x_i))\}_{i=1}^{m}$, the goal is to determine the parameter vector $\theta$ that minimizes a loss function $L$:

\begin{equation}
    \arg\min_{\theta} \frac{1}{m} \sum_{i=1}^{m} L(h_{\theta}(x_i), y_i^*)
\end{equation}

where $y_i^*$ are the ground-truth values (i.e., target outputs for regression tasks) and $L$ is a predefined loss function, 
such as the $L_1$ or $L_2$ loss, depending on the nature of $y$.

\subsection{Workflow of HADA}

The HADA framework follows a structured three-phase approach:

\begin{enumerate}
    \item \textbf{Data Collection (Benchmarking)}: In this initial phase, multiple runs of the target algorithm are conducted across 
    different configurations, varying both hyperparameters and hardware settings. The purpose is to gather empirical performance data 
    that will serve as the basis for training surrogate models.
    \item \textbf{Surrogate Model Training}: Once the dataset $\mathcal{S}$ is constructed, ML models are trained on the collected 
    data to learn the performance patterns. These models approximate the function $h_{\theta}(x)$ and are subsequently encoded into 
    a structured optimization problem following the EML paradigm.
    \item \textbf{Optimization}: The final phase involves solving the optimization problem, where user-defined constraints and an 
    objective function are applied on top of the learned surrogate models and expert-defined constraints. The solver searches for 
    the best combination of algorithmic and hardware configurations that meet the specified constraints while minimizing the 
    objective function (e.g., energy consumption or execution time).
\end{enumerate}

\subsection{Extending HADA for Energy and Carbon Footprint Optimization}

The core functionalities of \textit{Surrogate Model Training} and \textit{Optimization} are already implemented in the existing 
HADA prototype, which will be detailed in Chapter 5. Our primary contribution in this work is to extend HADA by incorporating 
additional metrics—\textbf{Resource Utilization, Energy Consumption, and Carbon Emissions}—into the empirical modeling process.

To achieve this, we systematically execute a variety of algorithms under diverse configurations (both in terms of hyperparameters 
and hardware platforms) while recording key performance metrics. These metrics serve as the target variables for the surrogate 
models, allowing us to model the energy footprint and environmental impact of computational workloads. Before implementing this 
extension, however, it is necessary to establish a framework for accurately measuring the Carbon Footprint of an AI algorithm. 
This is the focus of the next section.

\section{Carbon Footprint of Computation}

The carbon footprint of an algorithm is determined by two main factors: (1) the energy required to execute it and (2) the emissions 
produced in generating that energy. The first factor depends on computing resource usage—such as the number of processing cores, 
execution time, and data center efficiency—while the second, known as \textbf{Carbon Intensity}, is influenced by the energy
 production methods and geographic location. 

\textbf{Carbon Intensity} quantifies the greenhouse gas (GHG) emissions associated with electricity production, expressed in terms 
of carbon dioxide equivalent ($\mathsf{CO_2e}$). This metric accounts for the global warming potential of various GHGs emitted over 
a given timeframe \cite{GreenAlgorithms}. As described in \cite{courty2024codecarbon}, the total carbon footprint of a computational 
task can be estimated using the following equation:

\begin{equation}
    \mathsf{C} = \mathsf{E} \times \mathsf{CI}
\end{equation}

where:

\begin{itemize}
    \item $\mathsf{E}$ represents the total energy consumed by the computational infrastructure, measured in kilowatt-hours (kWh).
    \item $\mathsf{CI}$ represents the carbon intensity of the electricity consumed, measured in grams of $\mathsf{CO_2e}$ per 
    kilowatt-hour (g$\mathsf{CO_2e}$/kWh).
\end{itemize}

\subsection{Measuring Carbon Footprint with CodeCarbon}

\textbf{CodeCarbon} is an open-source tool designed to estimate the carbon footprint of computational workloads. It achieves this 
by directly measuring energy consumption from key hardware components—CPU, GPU, and RAM—at regular intervals (default: every 15 
seconds). Additionally, it tracks execution time to compute the total electricity usage of the system.

A distinguishing feature of CodeCarbon is its ability to incorporate region-specific \textbf{Carbon Intensity} data to provide 
location-dependent emission estimates. Carbon intensity is computed as a weighted average of the emissions from various energy 
sources used to generate electricity. These sources can be categorized into:
\begin{itemize}
    \item \textbf{Fossil Fuels}: Coal, petroleum, and natural gas—each associated with specific carbon intensities, meaning a known 
    quantity of carbon dioxide is emitted per kilowatt-hour of electricity generated.
    \item \textbf{Renewable and Low-Carbon Energy}: Solar, wind, hydroelectricity, biomass, geothermal, and nuclear power—sources 
    with significantly lower or near-zero carbon emissions.
\end{itemize}

Depending on the platform and data availability, CodeCarbon retrieves carbon intensity information from various sources. For private 
infrastructures, it relies on datasets such as those provided by Our World In Data \cite{ember2024carbonintensity}. Figure
\ref{fig:carbon_intensity} illustrates the carbon intensity of electricity generation by country as of 2023.

\begin{figure}
    \includegraphics[width=\linewidth]{imgs/carbon-intensity-electricity.png}
    \caption{Carbon intensity of electricity generation, 2023. Source: \cite{ember2024carbonintensity}}
    \label{fig:carbon_intensity}
\end{figure}

By integrating real-time hardware usage monitoring with location-based carbon intensity data, CodeCarbon enables researchers 
and practitioners to quantify the environmental impact of their computations. This information can help guide decision-making, 
such as optimizing code efficiency, choosing lower-carbon data centers, or scheduling workloads during periods of higher renewable 
energy availability.

\section{Extending HADA with CodeCarbon}

\subsection{Energy Management Algorithms}

The first step in extending HADA to incorporate Carbon Intensity is to construct a dataset that includes carbon footprint data from 
algorithm execution. This requires integrating \textbf{CodeCarbon} into the computational workflow used for running algorithms. 

For this study, we analyzed two algorithms: \textbf{ANTICIPATE} and \textbf{CONTINGENCY}, originally introduced in 
\cite{ijcai2019p150, 10.1007/978-3-319-93031-2_8} and previously used in the HADA framework paper \cite{DEFILIPPO2022109199}. 
These algorithms belong to the domain of \textbf{Energy Management Systems} and are designed to compute the amount of energy 
that must be generated to meet a required load while minimizing the total energy cost over a daily time horizon. Both methods
consider the inherent uncertainty in energy demand and production.

\begin{itemize}
    \item \textbf{ANTICIPATE} is an \textit{online}, scenario-based anticipatory algorithm that adapts dynamically to uncertainty.
    \item \textbf{CONTINGENCY} is a hybrid \textit{offline/online} approach that first constructs (offline) a pool of solutions, 
    which is then used to guide an efficient online decision-making process.
\end{itemize}

Both methods are characterized by a tunable configuration parameter that can be adjusted based on performance constraints, 
particularly execution time and solution quality:
\begin{itemize}
    \item For \textbf{ANTICIPATE}, the configuration parameter is the number of considered \textit{scenarios} (\verb|nScenarios|).
    \item For \textbf{CONTINGENCY}, it is the number of \textit{traces} (\verb|nTraces|).
\end{itemize}

The goal of HADA is to learn the relationship between this configuration parameter and various performance metrics that characterize 
algorithm efficiency and resource usage. The original dataset used in HADA includes the following attributes:

\begin{itemize}
    \item \verb|nParam|: The configuration parameter of the algorithm (\verb|nScenarios| for ANTICIPATE, \verb|nTraces| for 
    CONTINGENCY).
    \item \verb|time(s)|: Execution time required to find a solution.
    \item \verb|sol(keuro)|: Cost of the obtained solution (expressed in thousand euros), serving as a measure of solution quality.
    \item \verb|memAvg(MB)|: Average memory usage of the algorithm (in MB).
\end{itemize}

\subsection{Incorporating Energy and Carbon Footprint Metrics}

To extend this approach, we integrated additional metrics related to \textbf{energy consumption} and \textbf{carbon footprint}, 
leveraging the capabilities of CodeCarbon. The following attributes were added to the dataset:

\begin{itemize}
    \item \verb|emissions|: Total $\mathsf{CO_2e}$ emissions generated during execution (kg).
    \item \verb|emission_rate|: Rate of $\mathsf{CO_2e}$ emissions per second (kg/s).
    \item \verb|cpu_energy|: Energy consumed by the CPU (kWh).
    \item \verb|ram_energy|: Energy consumed by RAM (kWh).
    \item \verb|tot_energy|: Total energy consumption (kWh), which is the sum of \verb|cpu_energy| and \verb|ram_energy|. If 
    applicable, it also includes GPU energy consumption.
    \item \verb|country| and \verb|region|: Geographic location where the computation took place, used to determine the carbon 
    intensity of electricity.
    \item \verb|cpu_count|: Number of CPU cores used.
\end{itemize}

Additionally, we implemented tracking for the peak memory usage of each execution, which we included in the dataset under the 
attribute:

\begin{itemize}
    \item \verb|memPeak(MB)|: Maximum memory usage (in MB) recorded during execution.
\end{itemize}

By incorporating these additional metrics, HADA can now model not only algorithm efficiency and solution quality but also the 
environmental impact of computational workloads. This enhancement allows for a more comprehensive optimization approach, balancing 
performance requirements with sustainability considerations.

\subsection{Min-Cut/Max-Flow Algorithms}

Following our initial experiments with ANTICIPATE and CONTINGENCY, which were already included in HADA, we sought to extend our 
study to a broader class of algorithms. We selected a set of algorithms used to solve the \textbf{Minimum Cut/Maximum Flow 
(Min-Cut/Max-Flow)} problem in graphs, a fundamental problem in combinatorial optimization with widespread applications in various 
domains, particularly in \textbf{Computer Vision}. 

The Min-Cut/Max-Flow problem involves computing the maximum amount of flow that can be sent from a designated source node to a 
ink node in a flow network, subject to capacity constraints on edges. The \textbf{Max-Flow} problem seeks to maximize the total 
flow, while the \textbf{Min-Cut} problem finds the smallest set of edges that, if removed, would disconnect the source from the sink. 
The Max-Flow problem and the Min-Cut problem are closely related, as stated by the \textbf{Max-Flow Min-Cut Theorem}, which asserts 
that the maximum flow value in a network is equal to the capacity of the minimum cut.

Min-Cut/Max-Flow algorithms are widely used in image segmentation, stereo vision, and other Computer Vision tasks where images are 
represented as graphs, and energy minimization techniques are applied to solve labeling problems. We referred to the work of Jensen 
et al. (2023) \cite{Jensen2023Maxflow}, which provides a comprehensive review of state-of-the-art Min-Cut/Max-Flow algorithms, 
evaluated on a large dataset of Computer Vision problems. 

\subsubsection{Selected Algorithms}

Based on the findings of Jensen et al., we focused on three well-known Min-Cut/Max-Flow algorithms, each representing different 
families of flow-based optimization methods:

\begin{itemize}
    \item \textbf{Boykov-Kolmogorov (BK)}
    \item \textbf{Excess Incremental Breadth First Search (EIBFS)}
    \item \textbf{Hochbaum's Pseudo-Flow (HPF)}
\end{itemize}

According to Jensen et al., Min-Cut/Max-Flow algorithms can be broadly classified into different families based on their approach 
to augmenting paths and flow computation. The \textbf{Boykov-Kolmogorov} (BK) algorithm \cite{bk2004maxflow} belongs to the \textbf{Augmenting Paths 
(AP)} family, which is one of the oldest methods for solving the Max-Flow problem. This family dates back to the classical 
\textbf{Ford-Fulkerson Algorithm} \cite{ford1956maxflow}, which introduced the concept of augmenting paths to iteratively 
increase the flow. BK extends this approach by introducing heuristic techniques to improve efficiency, making it particularly 
effective for Computer Vision applications.

On the other hand, \textbf{Excess Incremental Breadth First Search (EIBFS)} \cite{goldberg2015faster} and \textbf{Hochbaum’s 
Pseudo-Flow (HPF)} \cite{hochbaum2008pseudoflow} belong to the \textbf{Pseudoflow} family, which differs from augmenting path 
methods in the way it manages flow excess. These algorithms prioritize maintaining a valid preflow at all times and use alternative 
strategies to push flow across the network, often improving efficiency in large-scale instances. 

The main differences between these algorithm families lie in the order in which they traverse nodes while searching for augmenting 
paths and in their mechanisms for pushing flow along paths in the graph.

\subsubsection{Algorithm Variants Considered}

Unlike ANTICIPATE and CONTINGENCY, which have explicit tunable configuration parameters, these Min-Cut/Max-Flow algorithms can 
be implemented with different optimizations \cite{Jensen2023Maxflow}. We treated the specific implementation variant of each algorithm as a configuration 
parameter and considered the following versions:

\begin{itemize}
    \item \textbf{Boykov-Kolmogorov (BK) Variants:}
    \begin{itemize}
        \item \textbf{BK}: The reference implementation.
        \item \textbf{MBK}: An optimized version by Jensen et al., using indices instead of pointers to reduce memory footprint.
        \item \textbf{MBK-R}: A second optimized version that reorders arcs to ensure that all outgoing edges from a node are 
        stored contiguously in memory.
    \end{itemize}
    \item \textbf{Excess Incremental Breadth First Search (EIBFS) Variants:}
    \begin{itemize}
        \item \textbf{EIBFS}: A slightly modified version of the original EIBFS algorithm.
        \item \textbf{EIBFS-I}: A version that replaces pointers with indices to improve memory locality.
        \item \textbf{EIBFS-I-NR}: A version similar to EIBFS-I but without arc reordering.
    \end{itemize}
    \item \textbf{Hochbaum’s Pseudo-Flow (HPF) Variants:}
    \begin{itemize}
        \item \textbf{HPF-H-F}: Highest-label variant using FIFO buckets.
        \item \textbf{HPF-H-L}: Highest-label variant using LIFO buckets.
        \item \textbf{HPF-L-F}: Lowest-label variant using FIFO buckets.
        \item \textbf{HPF-L-L}: Lowest-label variant using LIFO buckets.
    \end{itemize}
\end{itemize}

\subsubsection{Energy and Carbon Footprint Monitoring}

Jensen et al. provided scripts to evaluate the runtime performance of these algorithms, measuring both initialization time and 
execution time. To extend this analysis with energy consumption and carbon footprint tracking, we modified their experimental 
setup by integrating \textbf{CodeCarbon}. 

Since the original implementations are written in C++, we wrapped the algorithm scripts with a Python interface to facilitate 
real-time monitoring. This wrapper was designed to:
\begin{itemize}
    \item Track memory usage throughout execution.
    \item Use CodeCarbon to measure CPU and RAM energy consumption.
    \item Record $\mathsf{CO_2e}$ emissions based on the energy consumption and Carbon Intensity of the execution environment.
\end{itemize}

Unlike the experiments with ANTICIPATE and CONTINGENCY, where solution quality was a critical performance metric, here we focus 
solely on computational efficiency. The primary metrics of interest for this study are execution time, memory consumption, energy 
consumption, and environmental impact. By incorporating these Min-Cut/Max-Flow algorithms into HADA, we aim to explore 
energy-efficient graph-based optimization techniques and further refine our approach to sustainable AI computation.

\chapter{Experimental Analysis}

\section{Benchmarking on Different Hardware Platforms}

To comprehensively evaluate the performance of the selected algorithms under varying computational environments, we conducted experiments on multiple hardware platforms. This approach allows 
us to better capture how changes in hardware configurations impact execution time, energy consumption, and carbon footprint.

\subsection{Experimental Setup}

The benchmark phase was carried out on the following computing environments:

\begin{itemize}
    \item \verb|mbp19|: A personal laptop running MacOS Ventura 13.6.7, equipped with:
    \begin{itemize}
        \item 1.4 GHz Quad-Core Intel Core i5 processor;
        \item 8 GB 2133 MHz LPDDR3 RAM.
    \end{itemize}
    
    \item \verb|PC|: A desktop computer running Ubuntu 24.4 LTS, featuring:
    \begin{itemize}
        \item 3.4 GHz Intel Core i7-4770 Quad-Core processor;
        \item 8 GB DDR3 RAM.
    \end{itemize}
    
    \item \verb|leonardo|: A high-performance computing (HPC) infrastructure, \textbf{Leonardo}, hosted by \textbf{CINECA}. Leonardo consists of 4992 computing nodes, distributed across 
    two primary partitions, i.e. the Booster Partition and the Data Centric General Purpose partition. For our benchmark, we will rely on the booster partition, which has the following 
    specifics:
    \begin{itemize}
        \item \textbf{Booster partition} – used for this study – with:
        \begin{itemize}
            \item 512 GB (8 × 64 GB DDR4 3200 MHz) RAM per node;
            \item Single-socket, 32-core Intel Xeon Platinum 8358 CPU, 2.60 GHz (Ice Lake) (110,592 total cores);
            \item 4× NVIDIA custom Ampere A100 GPUs with 64 GB HBM2e memory, NVLink 3.0 (200 GB/s).
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Benchmarking ANTICIPATE and CONTINGENCY}

For the first set of experiments, we executed \textbf{ANTICIPATE} and \textbf{CONTINGENCY} on a collection of 30 problem instances. These instances were sampled using a Gaussian statistical 
model, as proposed in \cite{DEFILIPPO2022109199}. 

Each algorithm was executed on every instance with a varying configuration parameter (i.e., the number of \textit{traces} for CONTINGENCY and \textit{scenarios} for ANTICIPATE), ranging 
from 1 to 100. These experiments were performed on both \verb|mbp19| and \verb|leonardo|. As a result, we generated a dataset containing:

\begin{equation}
    30 \times 100 \times 2 \times 2 = 12,000 \text{ entries}.
\end{equation}

This dataset captures execution time, memory consumption, energy usage, and carbon footprint across different algorithmic configurations and hardware platforms.

\subsection{Benchmarking Min-Cut/Max-Flow Algorithms}

To evaluate the performance of the selected \textbf{Min-Cut/Max-Flow} algorithms, we relied on the dataset provided by Jensen et al. (2023) \cite{Jensen2023Maxflow}. This dataset contains a 
diverse set of Min-Cut/Max-Flow problem instances from the field of Computer Vision, allowing for a thorough assessment of algorithmic efficiency in real-world applications. Jensen et al. also
provided reference implementations of the algorithms, which we used for benchmarking.

Since these implementations were written in \textbf{C++} (for BK and EIBFS) and \textbf{C} (for HPF), integrating energy consumption tracking posed a challenge. Currently, few tools are 
available for directly monitoring the energy footprint of compiled C/C++ code. % TODO: Expand this discussion in the Related Work section.

To address this issue, we developed a \textbf{Python wrapper script} that executes the C++/C implementations and integrates \textbf{CodeCarbon} for energy and emission tracking. The wrapper 
allows us to monitor:
\begin{itemize}
    \item Execution time and memory usage;
    \item CPU and RAM energy consumption;
    \item $\mathsf{CO_2e}$ emissions based on real-time energy monitoring and Carbon Intensity data.
\end{itemize}

\subsection{Execution Constraints and Limitations}

While our goal was to run the selected algorithms on all instances of the Jensen et al. dataset, several factors constrained the scope of our experiments:

\begin{itemize}
    \item \textbf{Compilation and Platform Limitations:} The BK and EIBFS implementations could not be compiled on \verb|mbp19| due to missing dependencies and a lack of support for MacOS. 
    To address this, we utilized an alternative system—\verb|PC|—running a lightweight Ubuntu distribution, enabling execution on additional hardware.
    
    \item \textbf{Memory Constraints:} Some problem instances in the dataset were too large to fit within the available system memory. Although it was technically possible to use a swap 
    partition to accommodate larger instances, this would have distorted memory usage measurements, as available monitoring tools record only RAM consumption and do not account for swapped memory.
    
    \item \textbf{Execution Time Considerations:} Given the extensive number of instances in the dataset, running all of them would have been prohibitively time-consuming. As a result, we 
    selected a representative subset of problem instances that allowed us to evaluate performance across a range of complexity levels without exceeding computational constraints.
\end{itemize}

By executing experiments across multiple hardware platforms and addressing challenges related to execution and monitoring, we constructed a dataset that provides insights into algorithmic 
efficiency, energy consumption, and carbon emissions. The following sections will analyze the results obtained from these experiments and explore the implications of integrating sustainability
metrics into algorithm selection and hardware dimensioning.

\subsection{ANTICIPATE and CONTINGENCY}

Following the completion of the benchmark phase, we analyze the resulting data to evaluate the carbon emissions and energy consumption of the ANTICIPATE and CONTINGENCY algorithms. This section 
presents a comparative analysis of their environmental impact across different hardware platforms.

\subsubsection{Carbon Emissions Analysis}

Figure \ref{fig:ant_cont_emissions} displays the carbon emissions for ANTICIPATE and CONTINGENCY, categorized by hardware platform. The data reveals that emissions tend to be higher for the 
ANTICIPATE algorithm, particularly when executed on the \verb|leonardo| platform. Overall, the measured values remain relatively small, except for one notable outlier exceeding 0.020 kg of 
$\mathsf{CO_2e}$. Most emission values fall within the range $[0,0.005]$ kg. This pattern also applies to energy consumption, given that carbon emissions are directly proportional to energy 
usage in the absence of variation in Carbon Intensity.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/emissions_ant_cont.png}
    \caption{CO$_2$ emissions for ANTICIPATE and CONTINGENCY across different platforms.}
    \label{fig:ant_cont_emissions}
\end{figure}

\subsubsection{Energy Consumption Analysis}

Similarly, Figure \ref{fig:ant_cont_energy} illustrates the energy consumption for ANTICIPATE and CONTINGENCY. The overall trend aligns with the emissions data, with ANTICIPATE exhibiting higher 
energy consumption than CONTINGENCY, particularly on \verb|leonardo|.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/energy_ant_cont.png}
    \caption{Energy consumption for ANTICIPATE and CONTINGENCY across different platforms.}
    \label{fig:ant_cont_energy}
\end{figure}

\subsubsection{Analysis Excluding Outliers}

To gain a clearer understanding of the underlying distribution of emissions, we computed the \textbf{Interquartile Range (IQR)} for the dataset, removing extreme outliers. The refined data, 
shown in Figure \ref{fig:ant_cont_emissions_iqr}, confirms our previous observations: emissions for ANTICIPATE exhibit a wider distribution, with a slightly higher mean on \verb|leonardo|. 
For CONTINGENCY, emissions are significantly higher on \verb|leonardo|, while on \verb|mbp19|, they remain relatively low and exhibit a narrow distribution.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/CO2_emissions_no_outliers.png}
    \caption{CO$_2$ emissions for ANTICIPATE and CONTINGENCY across platforms, after removing outliers.}
    \label{fig:ant_cont_emissions_iqr}
\end{figure}

A similar trend is observed in the energy consumption data after removing outliers, as shown in Figure \ref{fig:ant_cont_energy_iqr}. On \verb|mbp19|, the CPU is the dominant factor driving 
energy consumption for ANTICIPATE, while for CONTINGENCY, overall energy usage remains comparatively lower.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/energy_consumption_no_outliers.png}
    \caption{Energy consumption for ANTICIPATE and CONTINGENCY across platforms, after removing outliers.}
    \label{fig:ant_cont_energy_iqr}
\end{figure}

\subsubsection{Correlation Analysis}

To further investigate the factors influencing emissions and energy consumption, Figures \ref{fig:ant_corr_mat} and \ref{fig:cont_corr_mat} present the correlation matrices for ANTICIPATE and 
CONTINGENCY, respectively.

For ANTICIPATE (Figure \ref{fig:ant_corr_mat}), the variable exhibiting the highest correlation with carbon emissions, aside from direct energy consumption, is the \textbf{average memory usage}, 
with a correlation coefficient of $0.93$. This suggests that higher memory consumption is a key contributor to increased emissions. Additionally, CPU energy consumption and RAM energy consumption 
both exhibit strong correlations with emissions, further reinforcing the relationship between computational resource usage and environmental impact.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/ant_corr_mat.png}
    \caption{Correlation matrix for ANTICIPATE.}
    \label{fig:ant_corr_mat}
\end{figure}

In contrast, for CONTINGENCY (Figure \ref{fig:cont_corr_mat}), there are no particularly strong linear correlations between emissions and other performance metrics. The highest correlation with 
emissions is observed for \textbf{execution time} ($0.36$), indicating that longer runtimes slightly contribute to higher emissions. However, the overall relationships appear weaker compared 
to ANTICIPATE, suggesting that CONTINGENCY’s energy usage is less dependent on a single dominant factor.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/cont_corr_mat.png}
    \caption{Correlation matrix for CONTINGENCY.}
    \label{fig:cont_corr_mat}
\end{figure}

From the correlation analysis, we make the following key observations:
\begin{itemize}
    \item Carbon emissions are perfectly proportional to total energy consumption, as indicated by the correlation of $1.0$ between \verb|totEnergy| and \verb|CO2e|. This reflects the fact 
    that, in the absence of varying Carbon Intensity, emissions are solely determined by energy consumption.
    \item Memory usage plays a significant role in determining emissions for ANTICIPATE, with a correlation of $0.93$. This suggests that optimizing memory allocation could be an effective s
    trategy for reducing emissions.
    \item For CONTINGENCY, execution time has the highest correlation with emissions ($0.36$), indicating a weaker but notable relationship.
    \item CPU and RAM energy consumption are highly correlated with total emissions ($0.98$ and $0.99$, respectively). Since total energy consumption is simply the sum of CPU and RAM energy, 
    this is expected.
    \item Retaining energy consumption values in the dataset is beneficial as these values can be scaled based on different Carbon Intensity factors. This enables an analysis of the impact 
    of computing in different geographic locations.
\end{itemize}

\subsubsection{Testing the HADA Framework with the Expanded Datasets}

To further explore the optimization capabilities of HADA, we conducted additional experiments on ANTICIPATE and CONTINGENCY, incorporating constraints on runtime, memory usage, and 
solution cost. The goal was to determine whether HADA could effectively optimize carbon emissions while satisfying various computational constraints. 

The results for ANTICIPATE are presented in Table \ref{tab:anticipate_results}. We tested multiple combinations of constraints on solution cost (expressed in thousands of euros, k€) and 
average memory consumption (expressed in MB), while minimizing CO₂ emissions (expressed in kg). In cases where HADA was unable to find a valid solution satisfying the constraints, we 
marked the corresponding results as "none".

\begin{table}[h!]
    \centering
    \begin{tabular}{|cc|ccccc|}
        \hline
        \multicolumn{2}{|c|}{Bounds} & \multicolumn{5}{c|}{Solution} \\
        \hline
        Memory (MB) & Cost (k€) & $n^P$ & HW & CO$_2$ (kg) & Memory (MB) & Cost (k€) \\
        \hline
        no & no & 1 & mbp19 & $7.23 \times 10^{-6}$ & - & - \\
        80 & no & none & none & none & none & none \\
        no & $100$ & none & none & none & none & none \\
        100 & $300$ & none & none & none & none & none \\
        150 & $300$ & 64 & mbp19 & $3.75 \times 10^{-4}$ & $141.53$ & $299.46$ \\
        170 & $270$ & 97 & mbp19 & $5.70 \times 10^{-4}$ & $169.85$ & $267.39$ \\
        170 & $280$ & 87 & mbp19 & $5.10 \times 10^{-4}$ & $160.12$ & $276.86$ \\
        300 & $500$ & 1 & mbp19 & $7.23 \times 10^{-6}$ & $81.47$ & $368.73$ \\
        \hline
    \end{tabular}
    \caption{Experimental results for the ANTICIPATE algorithm with constraints on memory usage and solution cost.}
    \label{tab:anticipate_results}
\end{table}

The results indicate that in most cases, minimizing CO$_2$ emissions aligns with minimizing memory consumption. The smallest emissions were observed when HADA selected configurations with fewer
scenarios ($n^P$). This is consistent with our earlier observations, where ANTICIPATE’s carbon footprint was highly correlated with memory usage ($0.93$ correlation).

A notable outcome is that HADA always selected the mbp19 platform, even when higher values of $n^P$ were required. This suggests that Leonardo’s computational power does not necessarily 
provide an advantage when minimizing emissions under typical constraints.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/time_comparison.png}
    \caption{Time Comparison between ANTICIPATE and CONTINGENCY on leonardo and mbp19}
    \label{fig:time_comparison}
\end{figure}

\ref{fig:time_comparison} shows results relative to the runtime of the two algorithms on both platforms. We can notice that for ANTICIPATE executes computations faster.
Nonetheless, its energy consumption per unit of work is significantly higher, resulting in greater CO$_2$ emissions overall. In contrast, mbp19, despite longer runtimes, consumed much less energy and emitted substantially less CO₂. This trade-off highlights 
that faster computation does not always equate to lower environmental impact. We can try to verify this by perfoming some tests with the runtime minization as objective, by trying to minimize
solution cost and emissions of CO2e.

\begin{table}[h!]
    \centering
    \begin{tabular}{|cc|ccccc|}
        \hline
        \multicolumn{2}{|c|}{Bounds} & \multicolumn{5}{c|}{Solution} \\
        \hline
        CO2eq (kg) & Cost (k€) & $n^P$ & HW & CO$_2$ (kg) & Time (s) & Cost (k€) \\
        \hline
        0.005 & 300 & 60 & leonardo & $2.94 \times 10^{-3}$ & $62.63$ & $299.92$ \\
        0.01 & 280 & 87 & leonardo & $4.24 \times 10^{-3}$ & $92.04$ & $274.01$ \\
        0.1 & 270 & 96 & leonardo & $4.74 \times 10^{-3}$ & $100.94$ & $267.40$ \\
        0.0001 & 400 & 1 & leonardo & $7.53 \times 10^{-5}$ & $1.24$ & $368.73$ \\
        0.00001 & 400 & 1 & mbp19 & $7.23 \times 10^{-6}$ & $1.53$ & $368.73$ \\
        \hline
    \end{tabular}
    \caption{Experimental results for the ANTICIPATE algorithm, minimizing runtime with constraints on solution cost and CO2 emissions}
    \label{tab:anticipate_results_runtime}
\end{table}

From the experiments presented in \ref{tab:anticipate_results_runtime} we can see that when we try to minimize the runtime, and we have tight bounds on the solution cost, the preferrable choice 
is leonardo, with an high number of scenarios. When we tighten the bounds on carbon emissions and loosen the constraints on the solution cost the number of scenarios is brought down to 1. When
constraints on the carbon emissions are very low ($ 1 \times 10 ^ {-5}$ or less), HADA starts preferring mbp19 to leonardo, due to the greater energy efficiency for smaller values of the parameter,
at the expense of an higher solution cost.

Furthermore, in cases where stricter memory and solution cost constraints were applied, HADA was unable to find feasible solutions. This suggests that certain configurations may inherently 
require high memory usage and cannot be optimized solely based on emissions minimization.

For CONTINGENCY, Table \ref{tab:contingency_results} presents the results of optimizing CO₂ emissions under constraints on execution time and solution cost. Given that execution time 
showed the highest correlation with emissions ($0.47$), we focused on determining whether limiting runtime would impact emissions optimization.

\begin{table}[h!]
    \centering
    \begin{tabular}{|cc|ccccc|}
        \hline
        \multicolumn{2}{|c|}{Bounds} & \multicolumn{5}{c|}{Solution} \\
        \hline
        Time (s) & Cost (k€) & $n^P$ & HW & CO$_2$ (kg) & Time (s) & Cost (k€) \\
        \hline
        no & no & 2 & mbp19 & $1.29 \times 10^{-6}$ & - & - \\
        no & $350$ & 4 & mbp19 & $1.53 \times 10^{-6}$ & - & $338.23k$ \\
        10 & no & 2 & mbp19 & $1.29 \times 10^{-6}$ & $6.01$ & - \\
        10 & $320$ & none & none & none & none & none \\
        60 & $350$ & 4 & mbp19 & $1.29 \times 10^{-6}$ & $7.68$ & $338.23k$ \\
        120 & $320$ & 90 & mbp19 & $1.24 \times 10^{-5}$ & $119.47$ & $314.11k$ \\
        \hline
    \end{tabular}
    \caption{Experimental results for the CONTINGENCY algorithm with constraints on runtime and solution cost.}
    \label{tab:contingency_results}
\end{table}

The results for CONTINGENCY further reinforce the platform selection pattern seen in ANTICIPATE: HADA consistently favored mbp19 over Leonardo. This was expected, as we can see In
\ref{fig:time_comparison}, that CONTINGENCY runs significantly slower on Leonardo than on mbp19. By looking at the dataset, we can see that Leonardo incurred a large overhead ($\approx60$s) 
even for a single trace, whereas mbp19 handled one trace in $5$-$6$s. This trend continued as traces increased. Even by looking at the memory consumption we can see that mbp19 outperforms
leonardo(\ref{fig:mem_usage_comparison}), with actually no visible gains in terms of solution cost, as we can see by \ref{fig:cost_comparison}. 

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.4\linewidth}
        \includegraphics[width=\linewidth]{imgs/avg_mem_usage.png}
        \caption{Average memory usage}
        \label{fig:mem_usage_comparison}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\linewidth}
        \includegraphics[width=\linewidth]{imgs/cost_comparison.png}
        \caption{Cost comparison}
        \label{fig:cost_comparison}
    \end{subfigure}
    \caption{Performance Comparison}
    \label{fig:comparison}
\end{figure}

From the results obtained in both ANTICIPATE and CONTINGENCY experiments, it is evident that platform selection plays a critical role in emissions optimization. While Leonardo provides 
superior raw computational speed, its higher energy consumption leads to a greater carbon footprint, making it an unfavorable choice for emissions minimization in the tested workloads.

\subsection{Min-Cut/Max-Flow Algorithms}

Emissions of BK, EIBFS and HPF

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/max_flow_emissions.png}
    \caption{CO2 emissions for Min-Cut/Max-Flow algorithms}
    \label{fig:max_flow_emissions}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/max_flow_emissions_iqr.png}
    \caption{IQR for the CO2 emissions of Min-Cut/Max-Flow algorithms}
    \label{fig:max_flow_emissions_iqr}
\end{figure}

\begin{table}[h!]
    \centering
    \begin{tabular}{|ccc|cccc|}
        \hline
        \multicolumn{3}{|c|}{Inputs \& Bound} & \multicolumn{4}{c|}{Solution} \\
        \hline
        Nodes & Arcs & time(s) & Impl. & HW & CO2e & time(s) \\
        \hline
        40 & 400 & $0.1$ & MBK & pc & $4.38 \times 10^{-7}$ & $3.80 \times 10^{-5}$ \\
        120 & 2k & $0.1$ & MBK2 & pc & $4.40 \times 10^{-7}$ & $5.10 \times 10^{-5}$ \\
        1k & 5k & $0.1$ & MBK & pc & $4.40 \times 10^{-7}$ & $4.09 \times 10^{-4}$ \\
        10k & 62k & $0.1$ & BK & pc & $4.40 \times 10^{-7}$ & $2.63 \times 10^{-3}$ \\
        44k & 784k & $5$ & MBK2 & pc & $6.46 \times 10^{-6}$ & $1.39$ \\
        185k & 5M & $5$ & MBK2 & pc & $2.16 \times 10^{-6}$ & $0.28$ \\
        18M & 93M & $10$ & MBK2 & pc & $4.69 \times 10^{-5}$& $9.79$ \\
        143M & 1B & $10$ & MBK2 & pc & $4.69 \times 10^{-5}$ & $9.79$ \\
        \hline
    \end{tabular}
    \caption{Experimental results for the BK algorithm}
    \label{tab:bk_results}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{|ccc|cccc|}
        \hline
        \multicolumn{3}{|c|}{Inputs \& Bound} & \multicolumn{4}{c|}{Solution} \\
        \hline
        Nodes & Arcs & time(s) & Impl. & HW & CO2e & time(s) \\
        \hline
        40 & 400 & $0.1$ & EIBFS new & pc & $4.37 \times 10^{-7}$ & $3.75 \times 10^{-5}$ \\
        120 & 2k & $0.1$ & EIBFS new & pc & $4.40 \times 10^{-7}$ & $1.80 \times 10^{-4}$ \\
        1k & 5k & $0.1$ & EIBFS new & pc & $4.40 \times 10^{-7}$ & $2.03 \times 10^{-4}$ \\
        10k & 62k & $0.1$ & EIBFS old & pc & $4.40 \times 10^{-7}$ & $2.89 \times 10^{-3}$ \\
        44k & 784k & $0.5$ & EIBFS new & pc & $8.7 \times 10^{-7}$ & $0.27$ \\
        185k & 5M & $0.5$ & EIBFS new2 & pc & $1.30 \times 10^{-6}$ & $0.12$ \\
        18M & 93M & $5$ & EIBFS new & pc & $1.98 \times 10^{-5}$& $3.51$ \\
        143M & 1B & $10$ & none & none & none & none \\
        \hline
    \end{tabular}
    \caption{Experimental results for the EIBFS algorithm}
    \label{tab:eibfs_results}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{|ccc|cccc|}
        \hline
        \multicolumn{3}{|c|}{Inputs \& Bound} & \multicolumn{4}{c|}{Solution} \\
        \hline
        Nodes & Arcs & memPeak(MB) & Impl. & HW & CO2e & memPeak(MB) \\
        \hline
        2.5k & 50k & $10$ & HPF-H-F & mbp19 & $2.11 \times 10^{-6}$ & $0.48$ \\
        10k & 75k & $10$ & HPF-H-F & mbp19 & $4.91 \times 10^{-7}$ & $0.46$ \\
        100k & 300k & $100$ & HPF-L-L & mbp19 & $9.56 \times 10^{-7}$ & $15.46$ \\
        300k & 500k & $100$ & HPF-L-L & mbp19 & $1.46 \times 10^{-6}$ & $35.45$ \\
        1M & 5M & $100$ & none & none & none & none \\
        1M & 5M & $400$ & HPF-L-L & mpb19 & $8.25 \times 10^{-6}$ & $390.83$ \\
        10M & 25M & none & none & none & none \\
        10M & 25M & $3000$ & HPF-H-L & mbp19 & $6.85 \times 10^{-5}$ & $2122.97$ \\
        125M & 300M & $4000$ & HPF-L-F & mbp19 & $9.08 \times 10^{-3}$ & $3785.29$ \\
        \hline
    \end{tabular}
    \caption{Experimental results for the EIBFS algorithm}
    \label{tab:hpf_results}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/max_flow_corr_mat.png}
    \caption{Correlation matrix for Min-Cut/Max-Flow Algorithms BK, EIBFS and HPF}
    \label{fig:max_flow_corr_mat}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/bk_corr_mat.png}
    \caption{Correlation matrix for BK}
    \label{fig:bk_orr_mat}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/eibfs_corr_mat.png}
    \caption{Correlation matrix for EIBFS}
    \label{fig:eibfs_corr_mat}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/hpf_corr_mat.png}
    \caption{Correlation matrix for BK}
    \label{fig:hpf_flow_corr_mat}
\end{figure}

\chapter{HADA-as-a-Service}

\section{HADA Web Application}
Benchmark data was integrated into the HADA web application, requiring:
\begin{itemize}
\item Creation of JSON configuration files for each algorithm-hardware combination.
\item Specification of hyperparameters and performance targets.
\end{itemize}

Example JSON structure:
\begin{verbatim}
{
"name": "anticipate",
"HW_ID": "macbook",
"hyperparams": [
{"ID": "num_scenarios", "type": "int", "LB": 1, "UB": 100}
],
"targets": [
{"ID": "time", "LB": null, "UB": null},
{"ID": "memory", "LB": null, "UB": null},
{"ID": "emissions", "LB": null, "UB": null}
]
}
\end{verbatim}

\chapter{Conclusions}

This work extends HADA by integrating carbon emission constraints, enhancing its applicability 
for sustainable AI hardware selection. Through experimental benchmarks on laptops and HPC systems, 
we validated the framework’s ability to balance performance and environmental impact. The web-based prototype 
enables users to make informed decisions when configuring AI workloads under sustainability constraints.

\appendix

\printbibliography[heading=bibintoc] % biblatex

\acknowledgements
I'm very grateful to the inventor of the Prolog language, without whom this thesis couldn't exist. I'd also like 
to acknowledge my advisor Prof. Mario Rossi by tail-recursively acknowledging my advisor.
	
\end{document}